<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GIT</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <style>
    .line{ /* Describes only positioning behaviour */
      display: block; /* Not important, but helpful in this case */
      clear: both;    /* Not important, but helpful in this case */
    }
    .line__figure{ /* Describes only positioning behaviour */
      float:left;
    }
    .figure{ /* Describes only view representation. */
      display: block; /* Not important, but helpful in this case */
    }
    .figure__image{
      background: lightgray;
      width: 400px;
      height: 300px;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Rendering Graphs for Graph Reasoning <br> in Multimodal Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=QkcrPzIAAAAJ&hl=zh-CN&oi=sra" target="_blank">Yanbin Wei</a><sup>1,*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=SRNGm38AAAAJ&hl=zh-CN" target="_blank">Shuai Fu</a><sup>1,*</sup>,
            </span>
            <span class="author-block">
              <a href="https://wayson-ust.github.io/" target="_blank">Weisen Jiang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cse.ust.hk/~jamesk/" target="_blank">James T. Kwok</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://cse.sustech.edu.cn/faculty/~zhangy/" target="_blank">Yu Zhang</a><sup>1,3</sup>
            </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Southern University of Science and Technology,
               <sup>2</sup>Hong Kong University of Science and Technology
              <br><sup>3</sup>Peng Cheng Laboratory
              <br>*Equal Contribution</span>
            <span class="eql-cntrb"></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2402.02130.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p> Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on the training set achieve higher accuracy than the closed-source model GPT-4(V). We also study the effects of augmentations in graph reasoning. </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->





<!-- An Example Section -->
<section class="hero teaser">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title is-4 has-text-centered">An Example</h2>
            <center>
            <div class="container is-max-desktop">
            <div class="hero-body">
                <figure>
              <img src="static/images/motivation.png" alt="MY ALT TEXT" width="400"/>
                    </figure>
            </div>
                </center>
          </div>
        </div>
    </div>
</section>

<!-- GITQA Dataset Section -->
<section class="hero teaser">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title is-4 has-text-centered">GITQA Dataset</h2>
            <p class="has-text-centered">
                Please check out our GITQA dataset at <a href="https://huggingface.co/collections/Yanbin99/gitqa-datasets-65c705c9488606617e246bd3" target="_blank">GITQA Hugging Face Collection</a>.
            </p>
            <center>
            <img src="static/images/benchmark.png" alt="GITQA Benchmark" width="800"/>
            </center>
        </div>
    </div>
</section>

<!-- GITA Section -->
<section class="hero teaser">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title is-4 has-text-centered">GITA (Graph-Image-Text Assistant)</h2>
            <p class="has-text-centered">
               Please check out our Model Zoo for all public GITA checkpoints at <a href="https://huggingface.co/collections/Yanbin99/gita-65cacd0d9b312118de7bbe42" target="_blank">Model Checkpoints</a>.
            </p>
            <center>
            <img src="static/images/result1.png" alt="GITA Performance" width="600"/>
            <figcaption>Figure: Testing accuracy (averaged over eight graph reasoning tasks) of closed-source models GPT-4(V) and open-source models Vicuna/LLaVA.</figcaption>
            </center>
        </div>
    </div>
</section>

<!-- Modalities Models Illustration Section -->
<section class="hero teaser">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h3 class="title is-4 has-text-centered">Accuracy of Closed/Open-source Models Using Textual/Visual Modalities</h3>
            <center>
            <img src="static/images/result2.png" alt="Modalities Models Illustration" width="800"/>
            </center>
        </div>
    </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@TechReport{wei2024git,
  title={Rendering Graphs for Graph Reasoning in Multimodal Large Language Models},
  author={Wei, Yanbin and Fu, Shuai and Jiang, Weisen and Kwok, James T. and Zhang, Yu},
  type={Preprint},
  number={arXiv:2402.02130},
  year={2024}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- End of Statcounter Code -->

</body>
</html>
